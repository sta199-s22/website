---
title: "Intro to Linear Models"
date: "March 18 2022"
output:
  html_document:
    df_print: paged
---

```{r, warning = FALSE, message  = FALSE}
library(tidyverse)
library(tidymodels)
library(scatterplot3d)
```

# Bulletin
  
- Upcoming
  - [Project report](https://sta199-sp22-003.netlify.app/project/project.html)


# Today

By the end of today you will

-   Understand the language and notation of linear modeling. 
-   Use `tidymodels` to make inference under a linear regression model

To begin, let's load the data. Today we'll work with the Palmer penguin dataset.

In particular, we will focus on three variables: bill length, flipper length and body mass of various penguins.

```{r load-data}
data(penguins)

penguins %>% 
  select(bill_length_mm, flipper_length_mm, body_mass_g) %>%
  glimpse()
```

```{r visualize-measurements, message = FALSE, warning = FALSE, echo = FALSE}
penguins %>%
  ggplot(aes(x = bill_length_mm, y = body_mass_g)) +
  geom_point(col = "steelblue") + 
  theme_bw() + 
  labs(x = "Bill Length (mm)", y = "Body mass (g)", title = "Body mass vs bill length")
```


# The language of linear modeling and the dimension of data

What is a linear model?

A linear model is a simple way to mathematically model the relationship between two or more observed measurements. 

We designate one of our measurements to be an "outcome", (e.g. body mass) and other observations (1 or more) to be "predictors", e.g. bill length.

Simple example:

```{r draw-a-line, message = FALSE, warning = FALSE, echo = FALSE}
penguins %>%
  ggplot(aes(x = bill_length_mm, y = body_mass_g)) +
  geom_point(col = "steelblue") + 
  theme_bw() + 
  labs(x = "Bill Length (mm)", y = "Body mass (g)", title = "Body mass vs bill length") +
  geom_smooth(method = 'lm', se = FALSE, col = "red")
```


$$
\underbrace{y}_{\text{outcome}} = \beta_0 + \underbrace{x_1}_{\text{predictor}} \beta_1
$$
Vocabulary:

- $y$: "outcome", also called "response" or "dependent variable"
- $x$: "predictor" also called "regressors", "independent variables", "covariates", "features", "the data"
- $\beta_0$, $\beta_1$: "constants" i.e. fixed numbers. These are **population parameters**. $\beta_0$ often gets the special name "the intercept".

The objective of linear regression is to find the best estimates for parameters (betas) for our purposes (fitting the data, prediction, etc.)

There's no reason we couldn't have more predictors. For example...

```{r multiple-predictors, message = FALSE, warning = FALSE, echo = FALSE}
scatterplot3d(penguins[,c("bill_length_mm",
                          "flipper_length_mm",
                          "body_mass_g")],
              pch = 19, 
              color="steelblue",
              xlab = "Bill length (mm)", ylab = "Flipper length (mm)", zlab = "Body mass (g)") 
```

and associated linear model:

$$
y = \beta_0 + x_{1} \beta_1 + x_{2} \beta_2
$$

- $y$: body mass
- $x_1$: bill length
- $x_2$: flipper length

```{r 3d-linear-model, message = FALSE, warning = FALSE, echo = FALSE}
plot3d = scatterplot3d(penguins[,c("bill_length_mm",
                          "flipper_length_mm",
                          "body_mass_g")],
              pch = 19, 
              color="steelblue",
              xlab = "Bill length (mm)", ylab = "Flipper length (mm)", zlab = "Body mass (g)") 

model  = lm(penguins$body_mass_g ~ penguins$bill_length_mm + penguins$flipper_length_mm)
plot3d$plane3d(model, col = "darkred")

```

## Building intuition for higher dimensions

- offline example


## 2-D example

## Exercise 1

Let's find the equation of the line in the 2D example above (body mass vs bill length).

First, label your outcome and explanatory variable:

- $y$:

- $x$:


Next, modify the code from [the prepare](https://rstudio-education.github.io/datascience-box/course-materials/slides/u4-d01-language-of-models/u4-d01-language-of-models.html?panelset5=code6#26) to **fit a linear model** to the data, this means using the data to estimate the $\beta$s

Note: no need to plot or mess with residuals here.

```{r linear-model-1}
# code here
```


## Exercise 2

Once we fit the model to our data, we have the equation:

$$
\hat{y} = \hat{\beta_0} + x_1 \hat{\beta_1}
$$
where the hats remind us we don't know the true population parameter $\beta$ that would result from fitting the entire population.

$\hat{y}$ is the "predicted outcome" of our model.

Write the linear model out in $x$, $\hat{y}$ notation, replacing $\hat{\beta}$ with the fitted constants you found from the previous exercise.

## Exercise 3

Use the equation from the previous exercise to predict the body mass of a penguin with a bill length of 50 mm

```{r calculate-3}
# use R as a calculator here
```

If you were asked to predict the body mass of a penguin with a bill lenght of 70 mm, is this model appropriate? Why or why not?

## Exercise 4

Five penguins in the data actually have a bill length of 50 mm. What are the five residuals associated with these observations?

Remember: a residual $\epsilon$ is the difference between a fitted (predicted) value and the observed value from the data:

$$
\epsilon_i = \hat{y}_i - y_i
$$

```{r ex-4}
# code here
```


## For next time ...

[Click here](https://seeing-theory.brown.edu/regression-analysis/index.html#section1) to interact with an ordinary least squares (OLS) linear regression model.

Select I and move the data points around.

Describe what you see.

